{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dst Model:\n",
    "### Implemented model to describe and forecast the density function of Dst conditioned to many geomagnetic data\n",
    "This files contains all the implemented functions and code in order to desing and develop a model to describe and forecast the the density function of Dst conditioned to many geomagnetic data. A mixtured of linear models, a recurrent neural network and the idea of quantile regression, as well as real geomagnetic data, have been used to achive this goal. \n",
    "\n",
    "The following lines have been distributed as follows:\n",
    "1. Libraries \n",
    "2. Preprocessing\n",
    "3. Desing and implementation of the model\n",
    "4. Distribution computation and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from keras import preprocessing\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Embedding, SimpleRNN\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.metrics import MeanSquaredError\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import zscore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DST from the 4 ground-based observatories\n",
    "Since we have the data splited in four .txt files with a specific format, we proced as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_dst():\n",
    "    \"\"\"Function to import DST data from multiple files and return it as a DataFrame.\"\"\"\n",
    "    data = []\n",
    "    for index in range(1, 4):\n",
    "        import_one_file(index, data)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def import_one_file(index, data):\n",
    "    \"\"\"Function to read a single DST file and append its data to the provided list.\"\"\"\n",
    "    current_dir = os.path.dirname(os.path.abspath(__file__))
    "    file_path = os.path.join(current_dir, 'Data', f'DTS_{index}.txt')
    "
    "    with open(file_path, 'r') as file:\n",
    "        for row in file:\n",
    "            numbers = re.findall(r'-?\\d+', row)\n",
    "            year = int(numbers[0]) // 100\n",
    "            # Determine the full year (considering years > 56 as 19xx and others as 20xx)\n",
    "            if year > 56:\n",
    "                complete_year = 1900 + year\n",
    "            else:\n",
    "                complete_year = 2000 + year\n",
    "            # Extract relevant data and convert to integers\n",
    "            numbers = [complete_year] + [int(numbers[0]) - year * 100] + [int(numbers[1])] + numbers[4:30]\n",
    "            data.append(list(map(int, numbers)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_aux = import_dst()\n",
    "\n",
    "dst_aux.drop([27], axis=1, inplace=True)\n",
    "\n",
    "dst_aux2 = dst_aux.drop([0, 1, 2], axis=1)\n",
    "\n",
    "dst_aux2[\"Date\"] = dst_aux[0] * 10000 + dst_aux[1] * 100 + dst_aux[2]\n",
    "dst_aux2[\"Date\"] = pd.to_datetime(dst_aux2['Date'].astype(str), format='%Y%m%d')\n",
    "\n",
    "hours = [f\"{i:02d}:00:00\" for i in range(24)] + [\"Date\"]\n",
    "dst_aux2.columns = hours\n",
    "\n",
    "df_melted = pd.melt(dst_aux2, id_vars=[\"Date\"], var_name='Hour', value_name='dst')\n",
    "df_melted.sort_values(by=['Date', 'Hour'], inplace=True)\n",
    "\n",
    "dst_aux3 = df_melted\n",
    "dst_aux3[\"Hour\"] = pd.to_timedelta(dst_aux3[\"Hour\"])\n",
    "dst_aux3[\"timedelta\"] = dst_aux3[\"Date\"] + dst_aux3[\"Hour\"]\n",
    "\n",
    "dst = dst_aux3.drop([\"Hour\", \"Date\"], axis=1).set_index(\"timedelta\")\n",
    "\n",
    "dst.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other features from ACE and DSCOVR\n",
    "Notice that this time format is diferent. First of all, it is divided in three periods and the time is restarted to zero in each period. However, we know the exact real date that corresponds with this format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH2 = Path(\"C:/Users/anhin/Documents/UAB - Mates/4t/TFG/Data/kaggle\")\n",
    "\n",
    "sunspots = pd.read_csv(DATA_PATH2 / \"sunspots.csv\")\n",
    "sunspots.timedelta = pd.to_timedelta(sunspots.timedelta)\n",
    "sunspots.set_index([\"timedelta\"], inplace=True)\n",
    "\n",
    "solar_wind = pd.read_csv(DATA_PATH2 / \"solar_wind.csv\")\n",
    "solar_wind.timedelta = pd.to_timedelta(solar_wind.timedelta)\n",
    "solar_wind.set_index([\"timedelta\"], inplace=True)\n",
    "\n",
    "satellite_positions = pd.read_csv(DATA_PATH2 / \"satellite_positions.csv\")\n",
    "satellite_positions.timedelta = pd.to_timedelta(satellite_positions.timedelta)\n",
    "satellite_positions.set_index([\"timedelta\"], inplace=True)\n",
    "\n",
    "print(sunspots.head())\n",
    "print(solar_wind.head())\n",
    "print(satellite_positions.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the dataset in Train and Test\n",
    "For each period, we divide the data in two groups (80% for Train and 20% for Test). After this, we divide the Train dataset in other two diferent groups: Train and Validation (80%/20%). Since is a time serie, we don't split it randomly; we will split it by time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def period_division(df):\n",
    "    ''' \n",
    "    Splits the dataframe into three separate dataframes based on the period column values: train_a, train_b, and train_c. \n",
    "    It assumes that the dataframe has a column called 'period' with these values.\n",
    "    '''\n",
    "    return {\n",
    "        \"period_a\": df[df[\"period\"] == \"train_a\"].drop(\"period\", axis=1), \n",
    "        \"period_b\": df[df[\"period\"] == \"train_b\"].drop(\"period\", axis=1), \n",
    "        \"period_c\": df[df[\"period\"] == \"train_c\"].drop(\"period\", axis=1)\n",
    "    }\n",
    "\n",
    "def split_data(timeserie, per):\n",
    "    '''\n",
    "    Splits each period dataframe into training, validation, and test sets.\n",
    "    \n",
    "    Parameters:\n",
    "    - timeserie: Dictionary containing the dataframes for each period.\n",
    "    - per: Percentage for splitting the data (e.g., 0.8 for 80% training).\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary containing the training, validation, and test sets for each period.\n",
    "    '''\n",
    "    train = {}\n",
    "    validation = {}\n",
    "    test = {}\n",
    "    \n",
    "    for period in timeserie.keys():\n",
    "        total_minutes = len(timeserie[period])\n",
    "        division1 = int(total_minutes * per * per)  # 80% of the 80% for training\n",
    "        division2 = int(total_minutes * per)        # 80% for training\n",
    "        \n",
    "        # Adjust to the nearest 60 to ensure the splits are on hour boundaries\n",
    "        if division1 % 60 != 0:\n",
    "            division1 += (60 - division1 % 60)\n",
    "        if division2 % 60 != 0:\n",
    "            division2 += (60 - division2 % 60)\n",
    "        \n",
    "        # Ensure indices do not exceed the length of the timeserie\n",
    "        division1 = min(division1, total_minutes - 1)\n",
    "        division2 = min(division2, total_minutes - 1)\n",
    "\n",
    "        train[period] = timeserie[period].iloc[:division1]\n",
    "        validation[period] = timeserie[period].iloc[division1:division2]\n",
    "        test[period] = timeserie[period].iloc[division2:]\n",
    "        \n",
    "    return {\"train\": train, \"validation\": validation, \"test\": test}\n",
    "\n",
    "def period_concat(timeserie):\n",
    "    ''' \n",
    "    Concatenates the dataframes from different periods into a single dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    - timeserie: Dictionary containing the dataframes for each period.\n",
    "    \n",
    "    Returns:\n",
    "    - Concatenated dataframe.\n",
    "    '''\n",
    "    return pd.concat([timeserie[\"period_a\"], timeserie[\"period_b\"], timeserie[\"period_c\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_train, sw_val, sw_test = split_data(period_division(solar_wind), 0.8)\n",
    "\n",
    "sw_train_tot = period_concat(sw_train)\n",
    "sw_val_tot = period_concat(sw_val)\n",
    "sw_test_tot = period_concat(sw_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df1, df2, df3):\n",
    "    \"\"\"\n",
    "    Preprocesses the data by interpolating missing values, computing hourly statistics,\n",
    "    merging datasets, imputing missing values, and normalizing the data.\n",
    "    \n",
    "    Parameters:\n",
    "    - df1: DataFrame containing solar wind data with numeric features and a 'source' column.\n",
    "    - df2: DataFrame containing sunspots data.\n",
    "    - df3: DataFrame containing satellite positions data.\n",
    "    \n",
    "    Returns:\n",
    "    - normalized: DataFrame containing the normalized hourly features.\n",
    "    \"\"\"\n",
    "    # Use interpolation to imput the solar wind numeric features\n",
    "    columns_to_interpolate = [col for col in df1.columns if col not in ['source']]\n",
    "    df1[columns_to_interpolate] = df1[columns_to_interpolate].interpolate(method='time')\n",
    "\n",
    "    # Compute the dayly mean and standard deviation for each feature, in order to transform the minutely data in hourly data\n",
    "    numeric_cols = df1.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "    mean_std_by_hour = df1[numeric_cols].resample('h').agg(['median', 'std'])\n",
    "\n",
    "    new_column_names = [f\"{col}_{stat}\" for col in mean_std_by_hour.columns.levels[0] for stat in mean_std_by_hour.columns.levels[1]]\n",
    "    mean_std_by_hour.columns = new_column_names\n",
    "\n",
    "    # Join the sunspots dataset\n",
    "    hourly_features = pd.merge(mean_std_by_hour, df2, on='timedelta', how='left')\n",
    "\n",
    "    # Impute the missing values filling with the lastest value \n",
    "    hourly_features[\"smoothed_ssn\"] = hourly_features['smoothed_ssn'].ffill()\n",
    "    hourly_features[\"smoothed_ssn\"] = hourly_features['smoothed_ssn'].bfill()\n",
    "\n",
    "    # Join the satellite positions dataset\n",
    "    hourly_features = pd.merge(hourly_features, df1[\"source\"], on='timedelta', how='left') # We join the source feature to decide which satellite use\n",
    "    hourly_features[\"source\"] = hourly_features[\"source\"].ffill() # We impute the missing data with the lastest value (?)\n",
    "    \n",
    "    df_aux = pd.merge(hourly_features, df3, on='timedelta', how='left') \n",
    "\n",
    "    hourly_features[\"gse_x\"] = df_aux.apply(lambda x: x[\"gse_x_ace\"] if x[\"source\"] == \"ac\" else x[\"gse_x_dscovr\"], axis=1).ffill()\n",
    "    hourly_features[\"gse_y\"] = df_aux.apply(lambda x: x[\"gse_y_ace\"] if x[\"source\"] == \"ac\" else x[\"gse_y_dscovr\"], axis=1).ffill()\n",
    "    hourly_features[\"gse_z\"] = df_aux.apply(lambda x: x[\"gse_z_ace\"] if x[\"source\"] == \"ac\" else x[\"gse_z_dscovr\"], axis=1).ffill()\n",
    "\n",
    "    hourly_features[\"gse_x\"] = df_aux.apply(lambda x: x[\"gse_x_ace\"] if x[\"source\"] == \"ac\" else x[\"gse_x_dscovr\"], axis=1).bfill()\n",
    "    hourly_features[\"gse_y\"] = df_aux.apply(lambda x: x[\"gse_y_ace\"] if x[\"source\"] == \"ac\" else x[\"gse_y_dscovr\"], axis=1).bfill()\n",
    "    hourly_features[\"gse_z\"] = df_aux.apply(lambda x: x[\"gse_z_ace\"] if x[\"source\"] == \"ac\" else x[\"gse_z_dscovr\"], axis=1).bfill()\n",
    "\n",
    "    hourly_features.drop(\"source\", axis=1, inplace=True)\n",
    "\n",
    "    # Normalize the values\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(hourly_features)\n",
    "\n",
    "    normalized = pd.DataFrame(\n",
    "        scaler.transform(hourly_features),\n",
    "        index=hourly_features.index,\n",
    "        columns=hourly_features.columns\n",
    "    )\n",
    "\n",
    "    return normalized\n",
    "\n",
    "def convert_in_real_time(df_list):\n",
    "    \"\"\"\n",
    "    Converts the indices of dataframes to real-time dates based on predefined start dates.\n",
    "    \n",
    "    Parameters:\n",
    "    - df_list: List of DataFrames to be converted.\n",
    "    \"\"\"\n",
    "    start_dates = [pd.to_datetime('1998-02-16 00:00:00'), pd.to_datetime('2013-06-01 00:00:00'), pd.to_datetime('2004-05-01 00:00:00')]\n",
    "    for i, df in enumerate(df_list):\n",
    "        df.index += start_dates[i]\n",
    "\n",
    "def add_dst(df_list, dst):\n",
    "    \"\"\"\n",
    "    Merges DST data with each DataFrame in the list and creates a column for one-hour earlier DST values.\n",
    "    \n",
    "    Parameters:\n",
    "    - df_list: List of DataFrames to be merged with DST data.\n",
    "    - dst: DataFrame containing DST data.\n",
    "    \"\"\"\n",
    "    for df in df_list:\n",
    "        df = pd.merge(dst, df, on='timedelta', how='right')\n",
    "        df.rename(columns={'dst': 'dst0'}, inplace=True)\n",
    "        \n",
    "        # Create one-hour earlier DST column\n",
    "        df[\"dst1\"] = df[\"dst0\"].shift(+1)\n",
    "        df.iloc[0, df.columns.get_loc(\"dst1\")] = df.iloc[0, df.columns.get_loc(\"dst0\")]\n",
    "        columns = df.columns.tolist()\n",
    "        columns.insert(1, columns.pop(columns.index(\"dst1\")))\n",
    "        df = df[columns]\n",
    "\n",
    "def final_timeserie(list_sw, sunspots, satellite_positions, dst):\n",
    "    \"\"\"\n",
    "    Preprocesses and merges datasets for each training, validation, and test period.\n",
    "    \n",
    "    Parameters:\n",
    "    - list_sw: List of dictionaries containing solar wind data for different periods.\n",
    "    - sunspots: DataFrame containing sunspots data.\n",
    "    - satellite_positions: DataFrame containing satellite positions data.\n",
    "    - dst: DataFrame containing DST data.\n",
    "    \n",
    "    Returns:\n",
    "    - timeseries: List of dictionaries containing preprocessed data for each period.\n",
    "    \"\"\"\n",
    "    timeseries = []\n",
    "    for sw_dict in list_sw:\n",
    "        df = [preprocessing(sw_dict[i], period_division(sunspots)[i], period_division(satellite_positions)[i]) for i in sw_dict.keys()]\n",
    "        convert_in_real_time(df)\n",
    "        add_dst(df, dst)\n",
    "\n",
    "        timeserie = {\"period_a\": df[0], \"period_b\": df[1], \"period_c\": df[2]}\n",
    "        timeseries.append(timeserie)\n",
    "\n",
    "    return timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_train, ts_val, ts_test = final_timeserie([sw_train, sw_val, sw_test], sunspots, satellite_positions, dst)\n",
    "\n",
    "ts_train_tot = period_concat(ts_train)\n",
    "ts_val_tot = period_concat(ts_val)\n",
    "ts_test_tot = period_concat(ts_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Visualization\n",
    "The distribution of some featuras are shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_plot(data):\n",
    "    \"\"\"\n",
    "    Plots a histogram of the provided data.\n",
    "\n",
    "    Parameters:\n",
    "    - data: Array-like, the data to plot.\n",
    "\n",
    "    Returns:\n",
    "    - None, displays the histogram plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 10/1.618))\n",
    "\n",
    "    plt.hist(data, bins=100, alpha=0.8, color='skyblue', edgecolor='skyblue', density=True)\n",
    "\n",
    "    plt.gca().spines['top'].set_linewidth(1.618)\n",
    "    plt.gca().spines['right'].set_linewidth(1.618)\n",
    "    plt.gca().spines['bottom'].set_linewidth(1.618)\n",
    "    plt.gca().spines['left'].set_linewidth(1.618)\n",
    "\n",
    "    plt.gca().ticklabel_format(style='plain', axis='y')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def qq_plot(data):\n",
    "    \"\"\"\n",
    "    Creates a Q-Q plot of the provided data, normalized to mean 0 and standard deviation 1.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: Array-like, the data to plot.\n",
    "    \n",
    "    Returns:\n",
    "    - None, displays the Q-Q plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 10/1.618))\n",
    "\n",
    "    # Normalize the data\n",
    "    data = (data - np.mean(data)) / np.std(data)\n",
    "\n",
    "    # Create the Q-Q plot\n",
    "    fig = sm.qqplot(data, line='45')\n",
    "\n",
    "    ax = plt.gca()\n",
    "\n",
    "    plt.setp(ax.get_lines()[1], 'color', 'tomato')  # Set the color of the reference line\n",
    "    line = ax.get_lines()[0]\n",
    "    line.set_markerfacecolor('skyblue')  # Set the marker face color\n",
    "    line.set_markeredgecolor('skyblue')  # Set the marker edge color\n",
    "    line.set_alpha(0.7)  # Set the marker transparency\n",
    "\n",
    "    ax.tick_params(axis='both', which='major', labelsize=7)\n",
    "\n",
    "    ax.xaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "    ax.yaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "\n",
    "    plt.gca().spines['top'].set_linewidth(1.618)\n",
    "    plt.gca().spines['right'].set_linewidth(1.618)\n",
    "    plt.gca().spines['bottom'].set_linewidth(1.618)\n",
    "    plt.gca().spines['left'].set_linewidth(1.618)\n",
    "\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "\n",
    "    plt.gca().ticklabel_format(style='plain', axis='y')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_plot(list(sw_train_tot[\"density\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_plot(list(sw_train_tot[\"bt\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_plot(list(sw_train_tot[\"temperature\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_plot(list(ts_train_tot[\"dst0\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qq_plot(list(ts_train_tot[\"dst0\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dessing and implementation of the model\n",
    "The model chosen is a recurrent neural network with quantile loss function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_config = {\n",
    "    \"batch_size\": 34,\n",
    "    \"timesteps\": 34\n",
    "}\n",
    "\n",
    "features_config = {\n",
    "    \"x_cols\": ['bx_gse_median', 'bx_gse_std', 'by_gse_median',\n",
    "       'by_gse_std', 'bz_gse_median', 'bz_gse_std', 'theta_gse_median',\n",
    "       'theta_gse_std', 'phi_gse_median', 'phi_gse_std', 'bx_gsm_median',\n",
    "       'bx_gsm_std', 'by_gsm_median', 'by_gsm_std', 'bz_gsm_median', 'bz_gsm_std',\n",
    "       'theta_gsm_median', 'theta_gsm_std', 'phi_gsm_median', 'phi_gsm_std',\n",
    "       'bt_median', 'bt_std', 'density_median', 'density_std', 'speed_median',\n",
    "       'speed_std', 'temperature_median', 'temperature_std', 'smoothed_ssn',\n",
    "       'gse_x', 'gse_y', 'gse_z', 'dst1'],\n",
    "\n",
    "    \"y_cols\": ['dst0']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QuantileLoss(perc, delta=1e-4):\n",
    "    \"\"\"\n",
    "    Creates a quantile loss function for use in training neural networks.\n",
    "    \n",
    "    Parameters:\n",
    "    - perc: List or array of quantiles.\n",
    "    - delta: Small value to ensure numerical stability (default is 1e-4).\n",
    "    \n",
    "    Returns:\n",
    "    - _qloss: The quantile loss function.\n",
    "    \"\"\"\n",
    "    perc = np.array(perc).reshape(-1)\n",
    "    perc.sort()\n",
    "    perc = perc.reshape(1, -1)\n",
    "    \n",
    "    def _qloss(y, pred):\n",
    "        \"\"\"\n",
    "        The quantile loss function.\n",
    "        \n",
    "        Parameters:\n",
    "        - y: True values.\n",
    "        - pred: Predicted values.\n",
    "        \n",
    "        Returns:\n",
    "        - loss: Computed quantile loss.\n",
    "        \"\"\"\n",
    "        y = tf.cast(y, tf.float32)\n",
    "        pred = tf.cast(pred, tf.float32)\n",
    "        I = tf.cast(y <= pred, tf.float32)       \n",
    "        d = K.abs(y - pred)\n",
    "        correction = I * (1 - perc) + (1 - I) * perc\n",
    "        huber_loss = K.sum(correction * tf.where(d <= delta, 0.5 * d**2 / delta, d - 0.5 * delta), -1)\n",
    "        q_order_loss = K.sum(K.maximum(0.0, pred[:, :-1] - pred[:, 1:] + 1e-6), -1)\n",
    "        \n",
    "        return huber_loss + q_order_loss\n",
    "    \n",
    "    return _qloss\n",
    "\n",
    "def timeserie_division(df, batch_size):\n",
    "    \"\"\"\n",
    "    Divides a time series dataframe into sequences suitable for training neural networks.\n",
    "\n",
    "    Parameters:\n",
    "    - df: Dictionary of DataFrames, each representing a different time series period.\n",
    "    - batch_size: The size of the batches to be used in training.\n",
    "\n",
    "    Returns:\n",
    "    - dataset: TensorFlow dataset containing the divided time series data.\n",
    "    \"\"\"\n",
    "    dataset = None\n",
    "\n",
    "    sequence_length = param_config[\"timesteps\"]\n",
    "    x_cols = features_config[\"x_cols\"]\n",
    "    y_cols = features_config[\"y_cols\"]\n",
    "\n",
    "    for key in df.keys():\n",
    "        dataset_div = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "            data=df[key][x_cols][:-sequence_length],     # Input sequences\n",
    "            targets=df[key][y_cols][sequence_length:],   # Corresponding targets\n",
    "            sequence_length=sequence_length,             # Length of each sequence\n",
    "            batch_size=batch_size,                       # Batch size\n",
    "            shuffle=False                                # Do not shuffle the data\n",
    "        )\n",
    "\n",
    "        if dataset is None:\n",
    "            dataset = dataset_div\n",
    "        else:\n",
    "            dataset = dataset.concatenate(dataset_div)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rnn = timeserie_division(ts_train, param_config[\"batch_size\"])\n",
    "validation_rnn = timeserie_division(ts_val, param_config[\"batch_size\"])\n",
    "test_rnn = timeserie_division(ts_test, param_config[\"batch_size\"])\n",
    "\n",
    "print(f\"Number of train batches: {len(train_rnn)}\")\n",
    "print(f\"Number of val batches: {len(validation_rnn)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model: Quantile Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = np.arange(0.001,1,0.001)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.SimpleRNN(256, return_sequences=False, input_shape=(param_config[\"timesteps\"], len(features_config[\"x_cols\"]))), \n",
    "    tf.keras.layers.Dense(units=len(quantiles)) \n",
    "])\n",
    "\n",
    "model.compile(optimizer ='adam',\n",
    "              loss = QuantileLoss(quantiles),\n",
    "              metrics = [QuantileLoss(quantiles)]\n",
    "              )\n",
    "\n",
    "model_fit = model.fit(train_rnn, \n",
    "          batch_size=param_config[\"batch_size\"],\n",
    "          epochs = 500,\n",
    "          verbose = 1,\n",
    "          shuffle = False,\n",
    "          validation_data = validation_rnn,\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Distribution computation and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(y_test, quantiles):\n",
    "    \"\"\"\n",
    "    Plots the distribution of y_test against quantiles with specified formatting.\n",
    "\n",
    "    Parameters:\n",
    "    y_test (array-like): Array of true values.\n",
    "    quantiles (array-like): Array of quantiles corresponding to the true values.\n",
    "\n",
    "    Returns:\n",
    "    - None, displays the distribution plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 10 / 1.618))\n",
    "\n",
    "    plt.plot(y_test, quantiles, 'o-', color=\"skyblue\")\n",
    "\n",
    "    plt.xlabel('Y')\n",
    "\n",
    "    plt.gca().spines['top'].set_linewidth(1.618)   \n",
    "    plt.gca().spines['right'].set_linewidth(1.618)  \n",
    "    plt.gca().spines['bottom'].set_linewidth(1.618) \n",
    "    plt.gca().spines['left'].set_linewidth(1.618) \n",
    "    plt.gca().ticklabel_format(style='plain', axis='y')\n",
    "    plt.show()\n",
    "\n",
    "def plot_density(y_test, quantiles, y_real=None, normal=None, density_function=None):\n",
    "    \"\"\"\n",
    "    Plots the distribution of quantile predictions and optionally marks a real value on the plot.\n",
    "\n",
    "    Parameters:\n",
    "    - y_test: Array-like, predicted quantile values.\n",
    "    - quantiles: Array-like, quantile levels corresponding to the predicted values.\n",
    "    - y_real: Optional, the real value to be marked on the plot (default is None).\n",
    "    - normal: Optional, a real normal density function with parameters normal=[a,b] is ploted (default is None).\n",
    "    - density_function: Optional, list with the function and its x-max value.\n",
    "\n",
    "    Returns:\n",
    "    - None, displays the distribution plot.\n",
    "    \"\"\"\n",
    "    heights = np.diff(quantiles) / np.diff(y_test)\n",
    "\n",
    "    start_points = y_test[:-1]\n",
    "    end_points = y_test[1:]\n",
    "\n",
    "    plt.figure(figsize=(10, 10/1.618))\n",
    "\n",
    "    for start, end, height in zip(start_points, end_points, heights):\n",
    "        center_x = (start + end) / 2\n",
    "        width = end - start\n",
    "        \n",
    "        plt.bar(center_x, height, width=width, color='skyblue', edgecolor='skyblue', alpha=0.7)\n",
    "\n",
    "    if y_real is not None:\n",
    "        plt.axvline(x=y_real, color='tomato', linestyle='--', linewidth=1.5)\n",
    "\n",
    "    if normal is not None:\n",
    "        x = np.linspace(normal[0] - 4*normal[1], normal[0] + 4*normal[1], 1000)\n",
    "        y = norm.pdf(x, normal[0], normal[1])\n",
    "        plt.plot(x, y, color='tomato')\n",
    "\n",
    "    if density_function is not None:\n",
    "        x_range = np.arange(min(y_test), density_function[1], 0.001)\n",
    "        plt.plot(x_range, density_function[0](x_range, density_function[2]), \"-\", color=\"tomato\")\n",
    "\n",
    "\n",
    "    plt.xlabel('Y')\n",
    "    \n",
    "    plt.gca().spines['top'].set_linewidth(1.618)\n",
    "    plt.gca().spines['right'].set_linewidth(1.618)\n",
    "    plt.gca().spines['bottom'].set_linewidth(1.618)\n",
    "    plt.gca().spines['left'].set_linewidth(1.618)\n",
    "    plt.xlim(left=np.min(y_test) - 10, right=np.max(y_test) + 10)\n",
    "    plt.show()\n",
    "\n",
    "def model_validation(model, data, quantiles, quantile_stride=10):\n",
    "    \"\"\"\n",
    "    Calculate the probabilities for given test inputs and targets using a trained model.\n",
    "\n",
    "    Parameters:\n",
    "    model (keras.Model): The trained model for predictions.\n",
    "    data (tf.data.Dataset): The dataset containing input-target pairs.\n",
    "    quantiles (array-like): The array of quantiles to use for comparison.\n",
    "    quantile_stride (int, optional): The stride for quantile slicing. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "    None, displays the distribution plot.\n",
    "    \"\"\"\n",
    "    probabilities = []\n",
    "\n",
    "    for inputs, targets in data:\n",
    "        for j in range(len(inputs)):\n",
    "            single_input = inputs[j:j+1]\n",
    "            single_target = targets[j]\n",
    "            single_prediction = model.predict(single_input)[0][::quantile_stride]\n",
    "\n",
    "            count = sum(quant < single_target.numpy()[0] for quant in single_prediction)\n",
    "\n",
    "            if count == len(quantiles):\n",
    "                probabilities.append(quantiles[count - 1])\n",
    "            else:\n",
    "                probabilities.append(quantiles[count])\n",
    "\n",
    "    plt.figure(figsize=(10, 10 / 1.618))\n",
    "    plt.hist(probabilities, bins=100, alpha=0.8, color='skyblue', edgecolor='skyblue', density=True)\n",
    "    \n",
    "    plt.gca().spines['top'].set_linewidth(1.618)\n",
    "    plt.gca().spines['right'].set_linewidth(1.618)\n",
    "    plt.gca().spines['bottom'].set_linewidth(1.618)\n",
    "    plt.gca().spines['left'].set_linewidth(1.618)\n",
    "    plt.gca().ticklabel_format(style='plain', axis='y')\n",
    "    plt.show()\n",
    "\n",
    "def qqplot_empirical(empirical_quantiles):\n",
    "    \"\"\"\n",
    "    Generates a Q-Q plot for given empirical quantiles against a normal distribution.\n",
    "\n",
    "    Parameters:\n",
    "    empirical_quantiles (array-like): Array of empirical quantiles to plot.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 10 / 1.618))\n",
    "\n",
    "    res = stats.probplot(empirical_quantiles, dist=\"norm\", sparams=(0, 0.001))\n",
    "\n",
    "    plt.scatter(res[0][0], res[0][1], color='skyblue', label='Empirical Quantiles')\n",
    "\n",
    "    plt.plot(res[0][0], res[1][0] * res[0][0] + res[1][1], color='tomato')\n",
    "\n",
    "    plt.gca().spines['top'].set_linewidth(1.618)\n",
    "    plt.gca().spines['right'].set_linewidth(1.618)\n",
    "    plt.gca().spines['bottom'].set_linewidth(1.618)\n",
    "    plt.gca().spines['left'].set_linewidth(1.618)\n",
    "    plt.gca().ticklabel_format(style='plain', axis='y')\n",
    "    plt.xlabel('Theoretical Quantiles')\n",
    "    plt.ylabel('Empirical Quantiles')\n",
    "    plt.show()\n",
    "\n",
    "def multiple_qqplot_empirical(data, model):\n",
    "    \"\"\"\n",
    "    Generates multiple Q-Q plots to compare the empirical quantiles of model predictions\n",
    "    against the theoretical quantiles of a normal distribution.\n",
    "\n",
    "    Parameters:\n",
    "    data (tf.data.Dataset): Dataset containing input features and targets.\n",
    "    model (tf.keras.Model): Trained model to make predictions.\n",
    "\n",
    "    Returns:\n",
    "    None: Displays the Q-Q plots.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 10/1.618))\n",
    "\n",
    "    # Iterate through the dataset\n",
    "    for inputs, targets in data.take(1):\n",
    "        for j in range(len(inputs)):\n",
    "            single_input = inputs[j:j+1]\n",
    "            single_prediction = model.predict(single_input)[0][::10]\n",
    "            single_prediction = zscore(single_prediction)\n",
    "\n",
    "            res = stats.probplot(single_prediction, dist=\"norm\", sparams=(0, 0.001))\n",
    "\n",
    "            plt.scatter(res[0][0], res[0][1], alpha=0.4, color=\"skyblue\")\n",
    "\n",
    "    plt.plot(res[0][0], res[1][0] * res[0][0] + res[1][1], color='tomato')\n",
    "\n",
    "    plt.gca().spines['top'].set_linewidth(1.618)\n",
    "    plt.gca().spines['right'].set_linewidth(1.618)\n",
    "    plt.gca().spines['bottom'].set_linewidth(1.618)\n",
    "    plt.gca().spines['left'].set_linewidth(1.618)\n",
    "    plt.gca().ticklabel_format(style='plain', axis='y')\n",
    "    plt.xlabel('Theoretical Quantiles')\n",
    "    plt.ylabel('Empirical Quantiles')\n",
    "    plt.show()\n",
    "\n",
    "def plot_linear_regression_on_points(single_prediction, stride=10, num_points=15):\n",
    "    \"\"\"\n",
    "    Plot the linear regression on points derived from single predictions.\n",
    "\n",
    "    Parameters:\n",
    "    single_prediction (array-like): The array of predictions from which points are derived.\n",
    "    stride (int, optional): The stride for slicing the predictions. Default is 10.\n",
    "    num_points (int, optional): The number of points to consider from the sliced predictions. Default is 15.\n",
    "    \n",
    "    Returns:\n",
    "    List with the two coeficients from de linear model.\n",
    "    \"\"\"\n",
    "    points = []\n",
    "    q = single_prediction[::stride][:num_points]\n",
    "\n",
    "    # Calculate the midpoints and their corresponding values\n",
    "    for i in range(len(q) - 1):\n",
    "        points.append((q[i] + (q[i + 1] - q[i]) / 2, 0.01 / (q[i + 1] - q[i])))\n",
    "\n",
    "    points_x, points_y = zip(*points)  # Unzip points into x and y components\n",
    "\n",
    "    # Log transform the y-values\n",
    "    points_y = np.log(points_y)\n",
    "\n",
    "    x = np.array(points_x).reshape(-1, 1)  # Convert to column matrix\n",
    "    y = points_y\n",
    "\n",
    "    # Initialize and fit the linear regression model\n",
    "    linear_model = LinearRegression()\n",
    "    linear_model.fit(x, y)\n",
    "\n",
    "    y_pred = linear_model.predict(x)\n",
    "\n",
    "    print(f\"Coefficient: {linear_model.coef_[0]}\")\n",
    "    print(f\"Intercept: {linear_model.intercept_}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 10 / 1.618))\n",
    "    plt.plot(points_x, points_y, 'o-', color=\"skyblue\", label='Data Points')\n",
    "    plt.plot(x, y_pred, color='tomato', label='Linear Model')\n",
    "    plt.ylabel('log f(y)')\n",
    "    plt.xlabel('y')\n",
    "    plt.gca().spines['top'].set_linewidth(1.618)\n",
    "    plt.gca().spines['right'].set_linewidth(1.618)\n",
    "    plt.gca().spines['bottom'].set_linewidth(1.618)\n",
    "    plt.gca().spines['left'].set_linewidth(1.618)\n",
    "    plt.show()\n",
    "\n",
    "    return [linear_model.intercept_,linear_model.coef_[0]]\n",
    "\n",
    "def multiple_plot_distribution(data, model, distribution_function=None):\n",
    "    \"\"\"\n",
    "    Generates Q-Q plots comparing model predictions against theoretical quantiles,\n",
    "    and plots the empirical distribution function.\n",
    "\n",
    "    Parameters:\n",
    "    test_rnn (tf.data.Dataset): Dataset containing input features and targets.\n",
    "    model (tf.keras.Model): Trained model to make predictions.\n",
    "    distribution_function(function, optional): Distribution function\n",
    "\n",
    "    Returns:\n",
    "    None: Displays the Q-Q plots.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 10/1.618))\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    # Iterate through the dataset\n",
    "    for inputs, targets in data.take(1):\n",
    "        for j in range(len(inputs)):\n",
    "            single_input = inputs[j:j+1]\n",
    "            single_prediction = model.predict(single_input)[0][::10]\n",
    "            single_prediction = zscore(single_prediction)\n",
    "\n",
    "            for k in range(len(single_prediction)):\n",
    "                all_results.append((single_prediction[k], 0.001+k*0.01))\n",
    "\n",
    "            plt.plot(single_prediction, np.arange(0.001, 1, 0.001)[::10], '-', color=\"skyblue\", alpha=0.4)\n",
    "\n",
    "    if distribution_function is not None:\n",
    "        # Fit a linear model to the results\n",
    "        linear_model = LinearRegression()\n",
    "        x,y = zip(*all_results)\n",
    "        x_values = np.array(x).reshape(-1, 1) \n",
    "        y_values = np.log(1/np.array(y) -1)\n",
    "        linear_model.fit(x_values, y_values)\n",
    "        \n",
    "        # Plot the empirical distribution function\n",
    "        y_range = np.arange(-5, 5, 0.01)\n",
    "        plt.plot(y_range, distribution_function(y_range, [linear_model.intercept_, linear_model.coef_[0]]), '-', color=\"tomato\")\n",
    "\n",
    "        print(f\"Coefficient: {linear_model.coef_[0]}\")\n",
    "        print(f\"Intercept: {linear_model.intercept_}\")\n",
    "\n",
    "    plt.xlabel('Y')\n",
    "\n",
    "    plt.gca().spines['top'].set_linewidth(1.618)   \n",
    "    plt.gca().spines['right'].set_linewidth(1.618)  \n",
    "    plt.gca().spines['bottom'].set_linewidth(1.618) \n",
    "    plt.gca().spines['left'].set_linewidth(1.618) \n",
    "    plt.gca().ticklabel_format(style='plain', axis='y')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single new observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "\n",
    "for inputs, targets in test_rnn.take(1):\n",
    "    single_input = inputs[i:(i+1)]  \n",
    "    single_target= targets[i]\n",
    "\n",
    "print(f\"Target Value: {single_target.numpy()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of the approximated distribution and denisty function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_prediction = model.predict(single_input)[0]\n",
    "plot_distribution(single_prediction[::10],quantiles[::10])\n",
    "plot_density(single_prediction[::10], quantiles[::10], single_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_validation(model, train_rnn, quantiles, quantile_stride=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_validation(model, test_rnn, quantiles, quantile_stride=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normality analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_density(single_prediction[::10], quantiles[::10], single_target=None, normal=[-85,23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qqplot_empirical(single_prediction[::10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Left tail adjustment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model_coefs = plot_linear_regression_on_points(single_prediction)\n",
    "\n",
    "def tail_density(y, coefs):\n",
    "    return np.exp(coefs[0] + coefs[1]*y)\n",
    "\n",
    "plot_density(single_prediction[::10], quantiles[::10], single_target=None, normal=None, density_function=[tail_density(), -105, linear_model_coefs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_qqplot_empirical(test_rnn, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution_function(y, coefs):\n",
    "    return (1/(1+np.exp(coefs[0]+coefs[1]*y)))\n",
    "\n",
    "multiple_plot_distribution(test_rnn, model, distribution_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density_function(y,coefs):\n",
    "    mu = -coefs[0]/coefs[1]\n",
    "    s = -1/coefs[1]\n",
    "    return (np.exp(-(y-mu)/s))/(s*(1+np.exp(-(y-mu)/s))**2)\n",
    "\n",
    "plot_density(single_prediction[::10], quantiles[::10], single_target=None, normal=None, density_function=density_function)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
